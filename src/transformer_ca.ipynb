{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from src.initializers import SingularValueInitializer\n",
    "from typing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CASVDDenseAdd(tf.keras.layers.Layer):\n",
    "    \"\"\"SVD based densely connected layer.\"\"\"\n",
    "    def __init__(self, units: int, rank: int, activation: str = 'relu', use_bias: bool = True):\n",
    "        super(CASVDDenseAdd, self).__init__()\n",
    "        # initialise parameters\n",
    "        self.units = units\n",
    "        self.rank = rank\n",
    "        self.activation = tf.keras.activations.get(activation)\n",
    "        self.use_bias = use_bias\n",
    "        \n",
    "        # initialise variables\n",
    "        self.u = None\n",
    "        self.s = None\n",
    "        self.v = None\n",
    "        self.w = None\n",
    "        self.bias = None\n",
    "\n",
    "    def build(self, input_shapes: List[tf.TensorShape]):\n",
    "        # unpack input shapes\n",
    "        assert len(input_shapes) == 2\n",
    "        input_shape, context_shape = input_shapes\n",
    "        # define shapes\n",
    "        u_shape = tf.TensorShape([input_shape[-1], self.rank])\n",
    "        s_shape = tf.TensorShape([self.rank])\n",
    "        v_shape = tf.TensorShape([self.units, self.rank])\n",
    "        w_shape = tf.TensorShape([context_shape[-1], self.rank])\n",
    "        bias_shape = tf.TensorShape([self.units])\n",
    "        # define initializers\n",
    "        o_initializer = tf.keras.initializers.Orthogonal()\n",
    "        s_initializer = SingularValueInitializer(input_shape[-1], self.units)\n",
    "        z_initializer = tf.keras.initializers.get('Zeros')\n",
    "        # define variables\n",
    "        self.u = self.add_weight(\"U\", shape=u_shape, dtype=tf.float32, initializer=o_initializer)\n",
    "        self.s = self.add_weight(\"S\", shape=s_shape, dtype=tf.float32, initializer=s_initializer)\n",
    "        self.v = self.add_weight(\"V\", shape=v_shape, dtype=tf.float32, initializer=o_initializer)\n",
    "        self.w = self.add_weight(\"W\", shape=w_shape, dtype=tf.float32, initializer=o_initializer)\n",
    "        if self.use_bias:\n",
    "            self.bias = self.add_weight(\"bias\", shape=bias_shape, dtype=tf.float32, initializer=z_initializer)\n",
    "\n",
    "    def call(self, inputs: List[tf.Tensor]):\n",
    "        assert len(inputs) == 2\n",
    "        data, context = inputs\n",
    "        s = tf.linalg.diag(self.s + context@self.w)\n",
    "        temp = tf.einsum('nr, brr->bnr', self.u, s)\n",
    "        kernel = tf.einsum('bnr,mr->bnm', temp, self.v)\n",
    "        outputs = tf.einsum('bn, bnm->bm', data, kernel) + self.bias\n",
    "        if self.use_bias:\n",
    "            outputs += self.bias\n",
    "        # activate\n",
    "        return self.activation(outputs)\n",
    "    \n",
    "\n",
    "def chi(var, grad, nu):\n",
    "    \"\"\"Calculate additive part of  phi = I + chi given a variable and it's gradient.\"\"\"\n",
    "    a = tf.concat([grad, var], axis=1)\n",
    "    b = tf.concat([var, -grad], axis=1)\n",
    "    skew = tf.transpose(b)@a\n",
    "    c = tf.eye(skew.shape[0]) + nu/2 * skew\n",
    "    skew_inv = tf.linalg.inv(c)\n",
    "    return -nu * a @skew_inv@tf.transpose(b)\n",
    "\n",
    "def batch_transpose(a):\n",
    "    return tf.transpose(a, [0, 2, 1])\n",
    "\n",
    "def batch_mul(a, b, transpose_a=False, transpose_b=False):\n",
    "    if transpose_a:\n",
    "        a = batch_transpose(a)\n",
    "    if transpose_b:\n",
    "        b = batch_transpose(b)\n",
    "    return tf.einsum('bik,bkj->bij', a, b)\n",
    "\n",
    "def batch_assembled_gradient(u, s, v, du, ds, dv, eps = 10e-8):\n",
    "    # Diagonal matrices for singular values\n",
    "    s_matrix = tf.linalg.diag(s)\n",
    "    ds_matrix = tf.linalg.diag(ds)\n",
    "    # Calculate D\n",
    "    s_inv = tf.linalg.diag((s + eps)**(-1))\n",
    "    d = du@s_inv\n",
    "    # Calculate A\n",
    "    i = tf.eye(ds_matrix.shape[-1], batch_shape=[s.shape[0]])\n",
    "    a = tf.where(i == 1., ds_matrix - tf.transpose(u)@d, 0.0)\n",
    "    # Calculate K\n",
    "    i_skew = tf.ones_like(s_matrix) - i\n",
    "    k = tf.where(i_skew == 0.0, 0.0,  (tf.expand_dims(s ** 2, axis=-1) - tf.expand_dims(s, axis=-2) ** 2 + eps) ** (-1))\n",
    "    # Calculate B\n",
    "    b = k * (tf.transpose(v)@dv - batch_mul(d, u@s_matrix, True, False))\n",
    "    # Calculate Q\n",
    "    q = d + u@(a + batch_mul(s_matrix, batch_transpose(b) + b, True, False))\n",
    "    # Return dw\n",
    "    return q@tf.transpose(v)\n",
    "\n",
    "def loss(model, inputs, targets):\n",
    "    outputs = model(inputs)\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(targets, outputs)\n",
    "\n",
    "def grad(model, inputs, targets):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss_value = loss(model, inputs, targets)\n",
    "    return loss_value, tape.gradient(loss_value, model.trainable_variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nBQuibYA4n0n"
   },
   "source": [
    "## Positional encoding\n",
    "\n",
    "Since this model doesn't contain any recurrence or convolution, positional encoding is added to give the model some information about the relative position of the words in the sentence. \n",
    "\n",
    "The positional encoding vector is added to the embedding vector. Embeddings represent a token in a d-dimensional space where tokens with similar meaning will be closer to each other. But the embeddings do not encode the relative position of words in a sentence. So after adding the positional encoding, words will be closer to each other based on the *similarity of their meaning and their position in the sentence*, in the d-dimensional space.\n",
    "\n",
    "See the notebook on [positional encoding](https://github.com/tensorflow/examples/blob/master/community/en/position_encoding.ipynb) to learn more about it. The formula for calculating the positional encoding is as follows:\n",
    "\n",
    "$$\\Large{PE_{(pos, 2i)} = sin(pos / 10000^{2i / d_{model}})} $$\n",
    "$$\\Large{PE_{(pos, 2i+1)} = cos(pos / 10000^{2i / d_{model}})} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "id": "WhIOZjMNKujn"
   },
   "outputs": [],
   "source": [
    "def get_angles(pos, i, d_model):\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
    "    return pos * angle_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "id": "1Rz82wEs5biZ"
   },
   "outputs": [],
   "source": [
    "def positional_encoding(position, d_model):\n",
    "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
    "                          np.arange(d_model)[np.newaxis, :],\n",
    "                          d_model)\n",
    "\n",
    "    # apply sin to even indices in the array; 2i\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "\n",
    "    # apply cos to odd indices in the array; 2i+1\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "\n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "id": "1kLCla68EloE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 50, 512)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pos_encoding = positional_encoding(50, 512)\n",
    "print (pos_encoding.shape)\n",
    "\n",
    "plt.style.use('dark_background')\n",
    "plt.pcolormesh(pos_encoding[0], cmap='RdBu')\n",
    "plt.xlabel('Depth')\n",
    "plt.xlim((0, 512))\n",
    "plt.ylabel('Position')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a_b4ou4TYqUN"
   },
   "source": [
    "## Masking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s42Uydjkv0hF"
   },
   "source": [
    "Mask all the pad tokens in the batch of sequence. It ensures that the model does not treat padding as the input. The mask indicates where pad value `0` is present: it outputs a `1` at those locations, and a `0` otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "id": "U2i8-e1s8ti9"
   },
   "outputs": [],
   "source": [
    "def create_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "\n",
    "    # add extra dimensions to add the padding\n",
    "    # to the attention logits.\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "id": "A7BYeBCNvi7n"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 1, 1, 5), dtype=float32, numpy=\n",
       "array([[[[0., 0., 1., 1., 0.]]],\n",
       "\n",
       "\n",
       "       [[[0., 0., 0., 1., 1.]]],\n",
       "\n",
       "\n",
       "       [[[1., 1., 1., 0., 0.]]]], dtype=float32)>"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.constant([[7, 6, 0, 0, 1], [1, 2, 3, 0, 0], [0, 0, 0, 4, 5]])\n",
    "create_padding_mask(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z0hzukDBgVom"
   },
   "source": [
    "The look-ahead mask is used to mask the future tokens in a sequence. In other words, the mask indicates which entries should not be used.\n",
    "\n",
    "This means that to predict the third word, only the first and second word will be used. Similarly to predict the fourth word, only the first, second and the third word will be used and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "id": "dVxS8OPI9uI0"
   },
   "outputs": [],
   "source": [
    "def create_look_ahead_mask(size):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask  # (seq_len, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "id": "yxKGuXxaBeeE"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 3), dtype=float32, numpy=\n",
       "array([[0., 1., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.random.uniform((1, 3))\n",
    "temp = create_look_ahead_mask(x.shape[1])\n",
    "temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xluDl5cXYy4y"
   },
   "source": [
    "## Scaled dot product attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "id": "LazzUq3bJ5SH"
   },
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "    \"\"\"Calculate the attention weights.\n",
    "    q, k, v must have matching leading dimensions.\n",
    "    k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
    "    The mask has different shapes depending on its type(padding or look ahead) \n",
    "    but it must be broadcastable for addition.\n",
    "\n",
    "    Args:\n",
    "    q: query shape == (..., seq_len_q, depth)\n",
    "    k: key shape == (..., seq_len_k, depth)\n",
    "    v: value shape == (..., seq_len_v, depth_v)\n",
    "    mask: Float tensor with shape broadcastable \n",
    "          to (..., seq_len_q, seq_len_k). Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "    output, attention_weights\n",
    "    \"\"\"\n",
    "\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "    # scale matmul_qk\n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "    # add the mask to the scaled tensor.\n",
    "    if mask is not None:\n",
    "        scaled_attention_logits += (mask * -1e9)  \n",
    "\n",
    "    # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
    "    # add up to 1.\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
    "\n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FiqETnhCkoXh"
   },
   "source": [
    "As the softmax normalization is done on K, its values decide the amount of importance given to Q.\n",
    "\n",
    "The output represents the multiplication of the attention weights and the V (value) vector. This ensures that the words you want to focus on are kept as-is and the irrelevant words are flushed out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "id": "n90YjClyInFy"
   },
   "outputs": [],
   "source": [
    "def print_out(q, k, v):\n",
    "    temp_out, temp_attn = scaled_dot_product_attention(\n",
    "      q, k, v, None)\n",
    "    print ('Attention weights are:')\n",
    "    print (temp_attn)\n",
    "    print ('Output is:')\n",
    "    print (temp_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "id": "yAzUAf2DPlNt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights are:\n",
      "tf.Tensor([[0. 1. 0. 0.]], shape=(1, 4), dtype=float32)\n",
      "Output is:\n",
      "tf.Tensor([[10.  0.]], shape=(1, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "temp_k = tf.constant([[10,0,0],\n",
    "                      [0,10,0],\n",
    "                      [0,0,10],\n",
    "                      [0,0,10]], dtype=tf.float32)  # (4, 3)\n",
    "\n",
    "temp_v = tf.constant([[   1,0],\n",
    "                      [  10,0],\n",
    "                      [ 100,5],\n",
    "                      [1000,6]], dtype=tf.float32)  # (4, 2)\n",
    "\n",
    "# This `query` aligns with the second `key`,\n",
    "# so the second `value` is returned.\n",
    "temp_q = tf.constant([[0, 10, 0]], dtype=tf.float32)  # (1, 3)\n",
    "print_out(temp_q, temp_k, temp_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "id": "zg6k-fGhgXra"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights are:\n",
      "tf.Tensor([[0.  0.  0.5 0.5]], shape=(1, 4), dtype=float32)\n",
      "Output is:\n",
      "tf.Tensor([[550.    5.5]], shape=(1, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# This query aligns with a repeated key (third and fourth), \n",
    "# so all associated values get averaged.\n",
    "temp_q = tf.constant([[0, 0, 10]], dtype=tf.float32)  # (1, 3)\n",
    "print_out(temp_q, temp_k, temp_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "id": "UAq3YOzUgXhb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights are:\n",
      "tf.Tensor([[0.5 0.5 0.  0. ]], shape=(1, 4), dtype=float32)\n",
      "Output is:\n",
      "tf.Tensor([[5.5 0. ]], shape=(1, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# This query aligns equally with the first and second key, \n",
    "# so their values get averaged.\n",
    "temp_q = tf.constant([[10, 10, 0]], dtype=tf.float32)  # (1, 3)\n",
    "print_out(temp_q, temp_k, temp_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aOz-4_XIhaTP"
   },
   "source": [
    "Pass all the queries together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "id": "6dlU8Tm-hYrF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights are:\n",
      "tf.Tensor(\n",
      "[[0.  0.  0.5 0.5]\n",
      " [0.  1.  0.  0. ]\n",
      " [0.5 0.5 0.  0. ]], shape=(3, 4), dtype=float32)\n",
      "Output is:\n",
      "tf.Tensor(\n",
      "[[550.    5.5]\n",
      " [ 10.    0. ]\n",
      " [  5.5   0. ]], shape=(3, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "temp_q = tf.constant([[0, 0, 10], [0, 10, 0], [10, 10, 0]], dtype=tf.float32)  # (3, 3)\n",
    "print_out(temp_q, temp_k, temp_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kmzGPEy64qmA"
   },
   "source": [
    "## Multi-head attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JPmbr6F1C-v_"
   },
   "source": [
    "Each multi-head attention block gets three inputs; Q (query), K (key), V (value). These are put through linear (Dense) layers and split up into multiple heads. \n",
    "\n",
    "The `scaled_dot_product_attention` defined above is applied to each head (broadcasted for efficiency). An appropriate mask must be used in the attention step.  The attention output for each head is then concatenated (using `tf.transpose`, and `tf.reshape`) and put through a final `Dense` layer.\n",
    "\n",
    "Instead of one single attention head, Q, K, and V are split into multiple heads because it allows the model to jointly attend to information at different positions from different representational spaces. After the split each head has a reduced dimensionality, so the total computation cost is the same as a single head attention with full dimensionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "id": "BSV3PPKsYecw"
   },
   "outputs": [],
   "source": [
    "class CAMultiHeadAttention(tf.keras.models.Model):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(CAMultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert d_model % self.num_heads == 0\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "        rank = int(d_model/2)\n",
    "        \n",
    "        self.wq = CASVDDenseAdd(d_model, rank)\n",
    "        self.wk = CASVDDenseAdd(d_model, rank)\n",
    "        self.wv = CASVDDenseAdd(d_model, rank)\n",
    "\n",
    "        self.dense = CASVDDenseAdd(d_model, rank)\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        \"\"\"Split the last dimension into (num_heads, depth).\n",
    "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
    "        \"\"\"\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, v, k, q, c_v, c_k, c_q, mask):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "        \n",
    "        seq_len_q = tf.shape(q)[1]\n",
    "        seq_len_k = tf.shape(k)[1]\n",
    "        seq_len_v = tf.shape(v)[1]\n",
    "        \n",
    "        q = tf.reshape(q, (batch_size * seq_len_q, -1))\n",
    "        k = tf.reshape(k, (batch_size * seq_len_k, -1))\n",
    "        v = tf.reshape(v, (batch_size * seq_len_v, -1))\n",
    "        \n",
    "        c_q = tf.reshape(c_q, (batch_size * seq_len_q, -1))\n",
    "        c_k = tf.reshape(c_k, (batch_size * seq_len_k, -1))\n",
    "        c_v = tf.reshape(c_v, (batch_size * seq_len_v, -1))  \n",
    "\n",
    "        q = self.wq((q, c_q))  # (batch_size * seq_len_q, d_model)\n",
    "        k = self.wk((k, c_k))  # (batch_size * seq_len_k, d_model)\n",
    "        v = self.wv((v, c_v))  # (batch_size * seq_len_v, d_model)\n",
    "        \n",
    "        q = tf.reshape(q, (batch_size, seq_len_q, self.d_model))\n",
    "        k = tf.reshape(k, (batch_size, seq_len_k, self.d_model))\n",
    "        v = tf.reshape(v, (batch_size, seq_len_v, self.d_model))\n",
    "\n",
    "        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
    "        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
    "\n",
    "        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
    "        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
    "            q, k, v, mask)\n",
    "\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
    "        \n",
    "        concat_attention = tf.reshape(scaled_attention, (batch_size * seq_len_q, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
    "        \n",
    "        output = self.dense((concat_attention, c_q))  # (batch_size, seq_len_q, d_model)\n",
    "        output = tf.reshape(output, (batch_size, seq_len_q, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
    "        \n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0D8FJue5lDyZ"
   },
   "source": [
    "Create a `MultiHeadAttention` layer to try out. At each location in the sequence, `y`, the `MultiHeadAttention` runs all 8 attention heads across all other locations in the sequence, returning a new vector of the same length at each location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "id": "Hu94p-_-2_BX"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([2, 60, 64]), TensorShape([2, 8, 60, 60]))"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_model = 64\n",
    "num_heads = 8\n",
    "\n",
    "temp_mha = CAMultiHeadAttention(d_model, num_heads)\n",
    "y = tf.random.uniform((2, 60, d_model))  # (batch_size, encoder_sequence, d_model)\n",
    "c = tf.random.uniform((2, 60, d_model))\n",
    "out, attn = temp_mha(v=y, k=y, q=y, c_q=c, c_k=c, c_v=c, mask=None)\n",
    "out.shape, attn.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RdDqGayx67vv"
   },
   "source": [
    "## Point wise feed forward network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gBqzJXGfHK3X"
   },
   "source": [
    "Point wise feed forward network consists of two fully-connected layers with a ReLU activation in between."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "id": "ET7xLt0yCT6Z"
   },
   "outputs": [],
   "source": [
    "class CAPointWiseFeedForward(tf.keras.models.Model):\n",
    "    def __init__(self, d_model, dff, activation: str = 'relu'):\n",
    "        super(CAPointWiseFeedForward, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.dff = dff\n",
    "        self.non_linear = CASVDDenseAdd(dff, int(dff/2), activation=activation)  # (batch_size * seq_len, dff)\n",
    "        self.linear = CASVDDenseAdd(d_model, int(d_model/2))  # (batch_size * seq_len, d_model)\n",
    "        \n",
    "    def call(self, inputs, context):\n",
    "        batch_size = inputs.shape[0]\n",
    "        seq_len = inputs.shape[1]\n",
    "        \n",
    "        inputs = tf.reshape(inputs, (batch_size * seq_len, -1))\n",
    "        context = tf.reshape(inputs, (batch_size * seq_len, -1))\n",
    "        \n",
    "        inputs = self.non_linear((inputs, context))\n",
    "        outputs = self.linear((inputs, context))\n",
    "        return tf.reshape(outputs, (batch_size, seq_len, self.d_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "id": "mytb1lPyOHLB"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 50, 64])"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_ffn = CAPointWiseFeedForward(64, 128)\n",
    "sample_ffn(tf.random.uniform((64, 50, 512)), tf.random.uniform((64, 50, 512))).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7e7hKcxn6-zd"
   },
   "source": [
    "## Encoder and decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MfYJG-Kvgwy2"
   },
   "source": [
    "The transformer model follows the same general pattern as a standard [sequence to sequence with attention model](nmt_with_attention.ipynb). \n",
    "\n",
    "* The input sentence is passed through `N` encoder layers that generates an output for each word/token in the sequence.\n",
    "* The decoder attends on the encoder's output and its own input (self-attention) to predict the next word. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QFv-FNYUmvpn"
   },
   "source": [
    "### Encoder layer\n",
    "\n",
    "Each encoder layer consists of sublayers:\n",
    "\n",
    "1.   Multi-head attention (with padding mask) \n",
    "2.    Point wise feed forward networks. \n",
    "\n",
    "Each of these sublayers has a residual connection around it followed by a layer normalization. Residual connections help in avoiding the vanishing gradient problem in deep networks.\n",
    "\n",
    "The output of each sublayer is `LayerNorm(x + Sublayer(x))`. The normalization is done on the `d_model` (last) axis. There are N encoder layers in the transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "id": "ncyS-Ms3i2x_"
   },
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.models.Model):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.mha = CAMultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = CAPointWiseFeedForward(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, context, training, mask):\n",
    "        attn_output, _ = self.mha(inputs, inputs, inputs, context, context, context, mask)  # (batch_size, input_seq_len, d_model)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        ffn_output = self.ffn(out1, context)  # (batch_size, input_seq_len, d_model)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        return out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "id": "AzZRXdO0mI48"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 43, 32])"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_encoder_layer = EncoderLayer(32, 8, 64)\n",
    "\n",
    "sample_encoder_layer_output = sample_encoder_layer(\n",
    "    tf.random.uniform((64, 43, 32)), tf.random.uniform((64, 43, 32)), False, None)\n",
    "\n",
    "sample_encoder_layer_output.shape  # (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6LO_48Owmx_o"
   },
   "source": [
    "### Decoder layer\n",
    "\n",
    "Each decoder layer consists of sublayers:\n",
    "\n",
    "1.   Masked multi-head attention (with look ahead mask and padding mask)\n",
    "2.   Multi-head attention (with padding mask). V (value) and K (key) receive the *encoder output* as inputs. Q (query) receives the *output from the masked multi-head attention sublayer.*\n",
    "3.   Point wise feed forward networks\n",
    "\n",
    "Each of these sublayers has a residual connection around it followed by a layer normalization. The output of each sublayer is `LayerNorm(x + Sublayer(x))`. The normalization is done on the `d_model` (last) axis.\n",
    "\n",
    "There are N decoder layers in the transformer.\n",
    "\n",
    "As Q receives the output from decoder's first attention block, and K receives the encoder output, the attention weights represent the importance given to the decoder's input based on the encoder's output. In other words, the decoder predicts the next word by looking at the encoder output and self-attending to its own output. See the demonstration above in the scaled dot product attention section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "id": "9SoX0-vd1hue"
   },
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.models.Model):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.mha1 = CAMultiHeadAttention(d_model, num_heads)\n",
    "        self.mha2 = CAMultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = CAPointWiseFeedForward(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout3 = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "    \n",
    "    def call(self, inputs, input_context, encoder_output, encoder_context, training, look_ahead_mask, padding_mask):\n",
    "        # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        attn1, attn_weights_block1 = self.mha1(inputs, inputs, inputs, input_context, input_context, input_context, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n",
    "        attn1 = self.dropout1(attn1, training=training)\n",
    "        out1 = self.layernorm1(attn1 + inputs)\n",
    "\n",
    "        attn2, attn_weights_block2 = self.mha2(\n",
    "            encoder_output, encoder_output, out1, encoder_context, encoder_context, input_context, padding_mask)  # (batch_size, target_seq_len, d_model)\n",
    "        attn2 = self.dropout2(attn2, training=training)\n",
    "        out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "        ffn_output = self.ffn(out2, input_context)  # (batch_size, target_seq_len, d_model)\n",
    "        ffn_output = self.dropout3(ffn_output, training=training)\n",
    "        out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "        return out3, attn_weights_block1, attn_weights_block2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "id": "Ne2Bqx8k71l0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 50, 32])"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_decoder_layer = DecoderLayer(32, 8, 64)\n",
    "\n",
    "sample_decoder_layer_output, _, _ = sample_decoder_layer(\n",
    "    tf.random.uniform((64, 50, 32)), tf.random.uniform((64, 50, 32)), sample_encoder_layer_output, tf.random.uniform((64, 43, 32)),\n",
    "    False, None, None)\n",
    "\n",
    "sample_decoder_layer_output.shape  # (batch_size, target_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SE1H51Ajm0q1"
   },
   "source": [
    "### Encoder\n",
    "\n",
    "The `Encoder` consists of:\n",
    "1.   Input Embedding\n",
    "2.   Positional Encoding\n",
    "3.   N encoder layers\n",
    "\n",
    "The input is put through an embedding which is summed with the positional encoding. The output of this summation is the input to the encoder layers. The output of the encoder is the input to the decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "id": "jpEox7gJ8FCI"
   },
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.models.Model):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
    "               maximum_position_encoding, rate=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, \n",
    "                                                self.d_model)\n",
    "\n",
    "\n",
    "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) \n",
    "                           for _ in range(num_layers)]\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "        \n",
    "    def call(self, inputs, context, training, mask):\n",
    "\n",
    "        seq_len = tf.shape(inputs)[1]\n",
    "\n",
    "        # adding embedding and position encoding.\n",
    "        inputs = self.embedding(inputs)  # (batch_size, input_seq_len, d_model)\n",
    "        inputs *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        inputs += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "        inputs = self.dropout(inputs, training=training)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            inputs = self.enc_layers[i](inputs, context, training, mask)\n",
    "\n",
    "        return inputs  # (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "id": "8QG9nueFQKXx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 62, 32)\n"
     ]
    }
   ],
   "source": [
    "sample_encoder = Encoder(num_layers=2, d_model=32, num_heads=8, \n",
    "                         dff=64, input_vocab_size=8500,\n",
    "                         maximum_position_encoding=10000)\n",
    "\n",
    "temp_input = tf.random.uniform((64, 62), dtype=tf.int64, minval=0, maxval=200)\n",
    "temp_input_context = tf.random.uniform((64, 62, 32))\n",
    "\n",
    "sample_encoder_output = sample_encoder(temp_input, temp_input_context, training=False, mask=None)\n",
    "\n",
    "print (sample_encoder_output.shape)  # (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p-uO6ls8m2O5"
   },
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZtT7PKzrXkNr"
   },
   "source": [
    " The `Decoder` consists of:\n",
    "1.   Output Embedding\n",
    "2.   Positional Encoding\n",
    "3.   N decoder layers\n",
    "\n",
    "The target is put through an embedding which is summed with the positional encoding. The output of this summation is the input to the decoder layers. The output of the decoder is the input to the final linear layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "id": "d5_d5-PLQXwY"
   },
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.models.Model):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size,\n",
    "               maximum_position_encoding, rate=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
    "\n",
    "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) \n",
    "                           for _ in range(num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "    def call(self, inputs, input_context, encoder_output, encoder_context, training, look_ahead_mask, padding_mask):\n",
    "\n",
    "        seq_len = tf.shape(inputs)[1]\n",
    "        attention_weights = {}\n",
    "\n",
    "        inputs = self.embedding(inputs)  # (batch_size, target_seq_len, d_model)\n",
    "        inputs *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        inputs += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "        inputs = self.dropout(inputs, training=training)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            inputs, block1, block2 = self.dec_layers[i](inputs, input_context, \n",
    "                                                   encoder_output, encoder_context,\n",
    "                                                   training,\n",
    "                                                   look_ahead_mask, \n",
    "                                                   padding_mask\n",
    "                                                  )\n",
    "\n",
    "            attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
    "            attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
    "\n",
    "        # x.shape == (batch_size, target_seq_len, d_model)\n",
    "        return inputs, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "id": "a1jXoAMRZyvu"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([64, 26, 32]), TensorShape([64, 8, 26, 62]))"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_decoder = Decoder(num_layers=2, d_model=32, num_heads=8, \n",
    "                         dff=64, target_vocab_size=8000,\n",
    "                         maximum_position_encoding=5000)\n",
    "\n",
    "temp_target = tf.random.uniform((64, 26), dtype=tf.int64, minval=0, maxval=200)\n",
    "temp_target_context = tf.random.uniform((64, 26, 32))\n",
    "\n",
    "output, attn = sample_decoder(temp_target,            \n",
    "                              temp_target_context,\n",
    "                              encoder_output=sample_encoder_output,\n",
    "                              encoder_context=temp_input_context,\n",
    "                              training=False,\n",
    "                              look_ahead_mask=None, \n",
    "                              padding_mask=None)\n",
    "\n",
    "output.shape, attn['decoder_layer2_block2'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y54xnJnuYgJ7"
   },
   "source": [
    "## Create the Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uERO1y54cOKq"
   },
   "source": [
    "Transformer consists of the encoder, decoder and a final linear layer. The output of the decoder is the input to the linear layer and its output is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "id": "PED3bIpOYkBu"
   },
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, \n",
    "               target_vocab_size, pe_input, pe_target, rate=0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.encoder = Encoder(num_layers, d_model, num_heads, dff, \n",
    "                               input_vocab_size, pe_input, rate)\n",
    "\n",
    "        self.decoder = Decoder(num_layers, d_model, num_heads, dff, \n",
    "                               target_vocab_size, pe_target, rate)\n",
    "\n",
    "        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
    "    \n",
    "    def call(self, inp, inp_c, tar, tar_c, training, enc_padding_mask, look_ahead_mask, dec_padding_mask):\n",
    "\n",
    "        enc_output = self.encoder(inp, inp_c, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n",
    "\n",
    "        # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
    "        dec_output, attention_weights = self.decoder(\n",
    "            tar, tar_c, enc_output, inp_c, training, look_ahead_mask, dec_padding_mask)\n",
    "\n",
    "        final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
    "\n",
    "        return final_output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "id": "tJ4fbQcIkHW1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 36, 8000])"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_transformer = Transformer(\n",
    "    num_layers=2, d_model=32, num_heads=8, dff=64, \n",
    "    input_vocab_size=8500, target_vocab_size=8000, \n",
    "    pe_input=10000, pe_target=6000)\n",
    "\n",
    "temp_input = tf.random.uniform((64, 38), dtype=tf.int64, minval=0, maxval=200)\n",
    "temp_input_context = tf.random.uniform((64, 38, 32))\n",
    "temp_target = tf.random.uniform((64, 36), dtype=tf.int64, minval=0, maxval=200)\n",
    "temp_target_context = tf.random.uniform((64, 36, 32))\n",
    "\n",
    "fn_out, _ = sample_transformer(temp_input, temp_input_context,\n",
    "                               temp_target, temp_target_context,\n",
    "                               training=True, \n",
    "                               enc_padding_mask=None, \n",
    "                               look_ahead_mask=None,\n",
    "                               dec_padding_mask=None)\n",
    "\n",
    "fn_out.shape  # (batch_size, tar_seq_len, target_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_masks(inp, tar):\n",
    "    # Encoder padding mask\n",
    "    enc_padding_mask = create_padding_mask(inp)\n",
    "\n",
    "    # Used in the 2nd attention block in the decoder.\n",
    "    # This padding mask is used to mask the encoder outputs.\n",
    "    dec_padding_mask = create_padding_mask(inp)\n",
    "\n",
    "    # Used in the 1st attention block in the decoder.\n",
    "    # It is used to pad and mask future tokens in the input received by \n",
    "    # the decoder.\n",
    "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
    "    dec_target_padding_mask = create_padding_mask(tar)\n",
    "    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
    "\n",
    "    return enc_padding_mask, combined_mask, dec_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {
    "id": "iJwmp9OE29oj"
   },
   "outputs": [],
   "source": [
    "# The @tf.function trace-compiles train_step into a TF graph for faster\n",
    "# execution. The function specializes to the precise shape of the argument\n",
    "# tensors. To avoid re-tracing due to the variable sequence lengths or variable\n",
    "# batch sizes (the last batch is smaller), use input_signature to specify\n",
    "# more generic shapes.\n",
    "\n",
    "# train_step_signature = [\n",
    "#     tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
    "#     tf.TensorSpec(shape=(None, None, None), dtype=tf.float32),\n",
    "#     tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
    "#     tf.TensorSpec(shape=(None, None, None), dtype=tf.float32),\n",
    "# ]\n",
    "\n",
    "def unpack(packed):\n",
    "    unpacked = []\n",
    "    for element in packed:\n",
    "        if hasattr(element, 'layers'):\n",
    "            for element in unpack(element.layers):\n",
    "                unpacked.append(element)\n",
    "        else:\n",
    "            unpacked.append(element)   \n",
    "    return unpacked\n",
    "\n",
    "def train_step(inp, inp_c, tar, tar_c):\n",
    "    tar_inp = tar[:, :-1]\n",
    "    tar_c_inp = tar_c[:, :-1, :]\n",
    "    tar_real = tar[:, 1:]\n",
    "\n",
    "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, _ = sample_transformer(inp, inp_c, \n",
    "                                     tar_inp, tar_c_inp,\n",
    "                                     True, \n",
    "                                     enc_padding_mask, \n",
    "                                     combined_mask, \n",
    "                                     dec_padding_mask)\n",
    "        loss = loss_function(tar_real, predictions)\n",
    "\n",
    "        gradients = tape.gradient(loss, sample_transformer.trainable_variables)    \n",
    "        # Make gradients and variables zip\n",
    "        grads_and_vars = zip(sample_transformer.trainable_variables, gradients)\n",
    "        # Get svd layers\n",
    "        svd_layers = [layer for layer in unpack([sample_transformer]) if 'svd' in layer.name]\n",
    "        # Optimize svd layers\n",
    "        for layer in svd_layers:\n",
    "            # Get layer components (U, S, V, W)\n",
    "            layer_variables = layer.trainable_variables[:-1]\n",
    "            # Get gradients and variables for components\n",
    "            (u, s, v, w) , (du, ds, dv, dw) = zip(*[(v, g) for v, g in grads_and_vars if v.name in [var.name for var in layer_variables]])\n",
    "            # Update gradients and variables zip to not contain these layer's components\n",
    "            grads_and_vars = [(v, g) for v, g in grads_and_vars if v.name not in [var.name for var in layer_variables]]\n",
    "            # Calculate orthogonal update\n",
    "            chi_u = chi(u, du, nu)\n",
    "            chi_v = chi(v, dv, nu)\n",
    "            u_update = u + chi_u @ u\n",
    "            v_update = v + chi_v @ v\n",
    "            # Context updated coefficients\n",
    "            c = tf.reshape(inp_c, (-1, w.shape[0]))\n",
    "            s_ = s + c@w\n",
    "            # calculate assembled gradient\n",
    "            dk = batch_assembled_gradient(u, s_, v, du, ds, dv)\n",
    "            # Calculate singular value updates\n",
    "            psi_u = tf.transpose(u)@chi_u@u\n",
    "            psi_v = tf.transpose(v)@chi_v@v\n",
    "            s_matrix = tf.linalg.diag(s_)\n",
    "            s_update_matrix = psi_u@s_matrix + (s_matrix + psi_u@s_matrix)@tf.transpose(psi_v)  - learning_rate * (\n",
    "                tf.transpose(u_update)@dk@v_update + tf.linalg.diag(c@dw)\n",
    "            )\n",
    "            # Update orthogonal matrices\n",
    "            u.assign_add(chi_u @ u)\n",
    "            v.assign_add(chi_v @ v)\n",
    "            # Update singular values\n",
    "            s.assign_add(tf.reduce_mean(tf.linalg.diag_part(s_update_matrix), axis=0))\n",
    "            # regular updates\n",
    "            w.assign_add(-learning_rate * dw)            \n",
    "        # Optimize other variables\n",
    "        for variable, gradient in grads_and_vars:\n",
    "            variable.assign_add(-learning_rate * gradient)\n",
    "\n",
    "    train_loss(loss)\n",
    "    # train_accuracy(accuracy_function(tar_real, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n",
    "\n",
    "\n",
    "def accuracy_function(real, pred):\n",
    "    accuracies = tf.equal(real, tf.argmax(pred, axis=2))\n",
    "\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    accuracies = tf.math.logical_and(mask, accuracies)\n",
    "\n",
    "    accuracies = tf.cast(accuracies, dtype=tf.float32)\n",
    "    mask = tf.cast(mask, dtype=tf.float32)\n",
    "    return tf.reduce_sum(accuracies)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {
    "id": "bbvmaKNiznHZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 31 Loss 8.1558 Accuracy 0.0000\n"
     ]
    }
   ],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((temp_input, temp_input_context, temp_target, temp_target_context)).batch(2)\n",
    "\n",
    "EPOCHS = 1\n",
    "nu = 10e-4\n",
    "learning_rate = 10e-4\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD()\n",
    "\n",
    "loss_object = tf.keras.losses.sparse_categorical_crossentropy\n",
    "\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.Mean(name='train_accuracy')\n",
    "\n",
    "\n",
    "# Get svd layers\n",
    "svd_layers = [layer for layer in unpack([sample_transformer]) if 'svd' in layer.name]\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    train_loss\n",
    "    train_accuracy.reset_states()\n",
    "\n",
    "    # inp -> portuguese, tar -> english\n",
    "    for batch, (inp, inp_c, tar, tar_c) in enumerate(train_dataset):\n",
    "        tar_inp = tar[:, :-1]\n",
    "        tar_c_inp = tar_c[:, :-1, :]\n",
    "        tar_real = tar[:, 1:]\n",
    "\n",
    "        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions, _ = sample_transformer(inp, inp_c, \n",
    "                                         tar_inp, tar_c_inp,\n",
    "                                         True, \n",
    "                                         enc_padding_mask, \n",
    "                                         combined_mask, \n",
    "                                         dec_padding_mask)\n",
    "            loss = loss_function(tar_real, predictions)\n",
    "\n",
    "            gradients = tape.gradient(loss, sample_transformer.trainable_variables)    \n",
    "        # Make gradients and variables zip\n",
    "        grads_and_vars = zip(gradients, sample_transformer.trainable_variables)\n",
    "        svd_indices = []\n",
    "        # Optimize svd layers\n",
    "        for layer in svd_layers:\n",
    "            # Get layer components (U, S, V, W)\n",
    "            layer_variables = layer.trainable_variables[:-1]\n",
    "            # variable indices\n",
    "            variable_indices = [idx for idx, v in enumerate(sample_transformer.trainable_variables) \n",
    "                                if v.name in [var.name for var in layer_variables]]\n",
    "            # Get gradients and variables for components\n",
    "            u, s, v, w = layer_variables\n",
    "            du, ds, dv, dw = np.array(gradients)[variable_indices]\n",
    "            svd_indices = np.concatenate([svd_indices, variable_indices])\n",
    "            # Calculate orthogonal update\n",
    "            chi_u = chi(u, du, nu)\n",
    "            chi_v = chi(v, dv, nu)\n",
    "            u_update = u + chi_u @ u\n",
    "            v_update = v + chi_v @ v\n",
    "            # Context updated coefficients depending on encoder or decoder\n",
    "            c = inp_c if 'encoder' in layer.name else tar_c_inp\n",
    "            c = tf.reshape(c, (-1, w.shape[0]))\n",
    "            s_ = s + c@w\n",
    "            # calculate assembled gradient\n",
    "            dk = batch_assembled_gradient(u, s_, v, du, ds, dv)\n",
    "            # Calculate singular value updates\n",
    "            psi_u = tf.transpose(u)@chi_u@u\n",
    "            psi_v = tf.transpose(v)@chi_v@v\n",
    "            s_matrix = tf.linalg.diag(s_)\n",
    "            s_update_matrix = psi_u@s_matrix + (s_matrix + psi_u@s_matrix)@tf.transpose(psi_v)  - learning_rate * (\n",
    "                tf.transpose(u_update)@dk@v_update + tf.linalg.diag(c@dw)\n",
    "            )\n",
    "            # Update orthogonal matrices\n",
    "            u.assign_add(chi_u @ u)\n",
    "            v.assign_add(chi_v @ v)\n",
    "            # Update singular values\n",
    "            s.assign_add(tf.reduce_mean(tf.linalg.diag_part(s_update_matrix), axis=0))\n",
    "            # regular updates\n",
    "            w.assign_add(-learning_rate * dw)            \n",
    "        # Optimize other variables\n",
    "        optimizer.apply_gradients([(g, v) for idx, (g,v) in enumerate(grads_and_vars) if idx not in svd_indices])\n",
    "            \n",
    "        train_loss(loss)\n",
    "        # train_accuracy(accuracy_function(tar_real, predictions))\n",
    "    \n",
    "    print ('Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}'.format(\n",
    "        epoch + 1, batch, train_loss.result(), train_accuracy.result()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "s_qNSzzyaCbD"
   ],
   "name": "transformer.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}